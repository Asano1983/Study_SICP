{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 word2vec\n",
    "\n",
    "## 3.4 CBOWモデルの実装\n",
    "\n",
    "CBOWの実装を行う。\n",
    "実装するニューラルネットワークは図3-19．\n",
    "SimpleCBOWという名前で実装する（次章では、これを改良したCBOWクラスを実装）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import MatMul,SoftmaxWithLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleCBOW:\n",
    "    def __init__(self, vocab_size, hidden_size):  # hidden_sizeは中間層のニューロン数\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 重みの初期化\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.in_layer0 = MatMul(W_in)\n",
    "        self.in_layer1 = MatMul(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # メンバ変数に単語の分散表現を設定\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        # contextsは3次元のNumpy配列で図3-18でいうと形状(6,2,7)の3次元配列\n",
    "        # 0番目の次元：要素数はミニバッチの数\n",
    "        # 1番目の次元：要素数はコンテキストのウィンドウサイズ分（両側あるのでwindow_sizeの2倍）\n",
    "        # 2番目の次元：one-hot化されたベクトル\n",
    "        h0 = self.in_layer0.forward(contexts[:, 0])\n",
    "        h1 = self.in_layer1.forward(contexts[:, 1])\n",
    "        h = (h0 + h1) * 0.5\n",
    "        score = self.out_layer.forward(h)\n",
    "        loss = self.loss_layer.forward(score, target)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ds = self.loss_layer.backward(dout)\n",
    "        da = self.out_layer.backward(ds)\n",
    "        da *= 0.5\n",
    "        self.in_layer1.backward(da)\n",
    "        self.in_layer0.backward(da)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：同じ重み（W_in）を２つのMatMulレイヤーで共有していることに注意。\n",
    "params配列には同じ重みが複数存在し、AdamやMomentumなどのオプティマイザの処理が本来の挙動と異なってしまう。\n",
    "Trainerクラスの内部ではパラメータの重複を取り除く（common/trainer.pyのremove_duplicate参照）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')  # 親ディレクトリのファイルをインポートするための設定\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
    "\n",
    "\n",
    "window_size = 1\n",
    "hidden_size = 5\n",
    "batch_size = 3\n",
    "max_epoch = 1000\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "\n",
    "model = SimpleCBOW(vocab_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vecs = model.word_vecs\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word, word_vecs[word_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 word2vecに関する補足\n",
    "\n",
    "### 3.5.1 CBOWモデルと確率\n",
    "\n",
    "P(A):Aという事象が起きる確率\n",
    "P(A,B):AとBが同時に起こる確率\n",
    "P(A|B):事後確率（Bという情報が与えられたときにAが起こる確率）\n",
    "\n",
    "w_1,w_2,...,w_Tという単語の列で表されるコーパスを考える。\n",
    "そして図3-22のようにt番目の単語に対して、ウィンドウサイズが1のコンテキストを考える。\n",
    "\n",
    "コンテキストとしてw_{t-1}とw_{t+1}が与えられたときに、\n",
    "ターゲットがw_tとなる確率を数式で表すと\n",
    "\n",
    "P(w_t|w_{t-1},w_{t+1})\n",
    "\n",
    "CBOWモデルの損失関数は、この確率に対してlogを取りマイナスを付けたものになる\n",
    "（交差エントロピーを最小化すること、one-hot化されていることを考える）。\n",
    "\n",
    "L = -log P(w_t|w_{t-1},w_{t+1})\n",
    "\n",
    "これは負の対数尤度と呼ばれる。\n",
    "これは１つのサンプルデータに関する損失関数であり、コーパス全体に拡張すると、損失関数は次のように書ける。\n",
    "\n",
    "L = -1/T \\sum_{t=1}^T log P(w_t|w_{t-1},w_{t+1})\n",
    "\n",
    "CBOWモデルの学習で行うことは、上の損失関数をできる限り小さくすることである。\n",
    "\n",
    "### 3.5.2 skip-gramモデル\n",
    "\n",
    "word2vecでは2つのモデルが提案されている。\n",
    "ひとつはCBOWモデル、もうひとつがskip-gram。\n",
    "\n",
    "skip-gramはCBOWで扱うコンテキストとターゲットを逆転させたモデル（図3-23参照）。\n",
    "skip-gramモデルのネットワーク構成は図3-24のようになる。\n",
    "\n",
    "確率の表記を使うと、skip-gramは\n",
    "\n",
    "P(w_{t-1}, w_{t+1}|w_t)\n",
    "\n",
    "をモデル化する。\n",
    "skip-gramモデルではコンテキストの単語の間に関連性がないこと（条件付き独立）を仮定し、次のように分解する。\n",
    "\n",
    "P(w_{t-1}, w_{t+1}|w_t) = P(w_{t-1}|w_t) P(w_{t+1}|w_t)\n",
    "\n",
    "### 3.5.3 カウントベース v.s. 推論ベース\n",
    "\n",
    "語彙に新しい単語を追加するとき、単語の分散表現を修正するとき\n",
    " - カウントベース：ゼロから計算しなおし\n",
    " - 推論ベース（word2vec）：これまでの学習結果を初期値として再学習可能\n",
    "\n",
    "得られる単語の分散表現の性質\n",
    " - カウントベース：単語の類似性\n",
    " - 推論ベース：単語の類似性に加えて、複雑な単語間パターン（king - man + woman = queenのような類推問題を解ける）。\n",
    " \n",
    "精度については優劣がつけられない。\n",
    "\n",
    "推論ベースの手法とカウントベースの手法には関連性があることが分かっている。\n",
    "具体的にはskip-gramと（次章で扱う）Negative Samplingを利用したモデルは、\n",
    "コーパス全体の共起行列に対して特殊な行列分解をしているのと同じであることが示されている。\n",
    "つまり、２つの世界はある条件において「つながっている」。\n",
    "\n",
    "さらにword2vec以降、推論ベースとカウントベースの手法を融合させたようなGloVeという手法も提案されている。\n",
    "その手法のアイデアは、コーパス全体の統計データの情報を損失関数に取り入れ、ミニバッチ学習をすることにある。\n",
    "それによって、２つの世界を明示的に融合させることに成功した。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
